{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1 — CSV ↔ Parquet Conversion & Basic EDA\n",
    "- Dataset: NYC Yellow Taxi 2023‑01 CSV  https://d37ci6vzurychx.cloudfront.net/tripdata/yellow_tripdata_2023-01.csv.gz\n",
    "- Load first 500 000 rows with Pandas for schema inspection and summary statistics\n",
    "(describe()).\n",
    "- Write the full file to Parquet with Snappy compression via Pandas (PyArrow backend).\n",
    "- Start a local Spark session, read the Parquet back, cache it, and verify matching row\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VendorID                          int64\n",
      "tpep_pickup_datetime     datetime64[us]\n",
      "tpep_dropoff_datetime    datetime64[us]\n",
      "passenger_count                 float64\n",
      "trip_distance                   float64\n",
      "RatecodeID                      float64\n",
      "store_and_fwd_flag               object\n",
      "PULocationID                      int64\n",
      "DOLocationID                      int64\n",
      "payment_type                      int64\n",
      "fare_amount                     float64\n",
      "extra                           float64\n",
      "mta_tax                         float64\n",
      "tip_amount                      float64\n",
      "tolls_amount                    float64\n",
      "improvement_surcharge           float64\n",
      "total_amount                    float64\n",
      "congestion_surcharge            float64\n",
      "airport_fee                     float64\n",
      "dtype: object\n",
      "            VendorID        tpep_pickup_datetime       tpep_dropoff_datetime  \\\n",
      "count   3.066766e+06                     3066766                     3066766   \n",
      "unique           NaN                         NaN                         NaN   \n",
      "top              NaN                         NaN                         NaN   \n",
      "freq             NaN                         NaN                         NaN   \n",
      "mean    1.730215e+00  2023-01-17 00:22:26.288164  2023-01-17 00:38:06.427874   \n",
      "min     1.000000e+00         2008-12-31 23:01:42         2009-01-01 14:29:11   \n",
      "25%     1.000000e+00  2023-01-09 16:21:57.250000         2023-01-09 16:37:06   \n",
      "50%     2.000000e+00  2023-01-17 08:42:29.500000  2023-01-17 08:58:30.500000   \n",
      "75%     2.000000e+00         2023-01-24 16:26:27         2023-01-24 16:42:49   \n",
      "max     2.000000e+00         2023-02-01 00:56:53         2023-02-02 09:28:47   \n",
      "std     4.438480e-01                         NaN                         NaN   \n",
      "\n",
      "        passenger_count  trip_distance    RatecodeID store_and_fwd_flag  \\\n",
      "count      2.995023e+06   3.066766e+06  2.995023e+06            2995023   \n",
      "unique              NaN            NaN           NaN                  2   \n",
      "top                 NaN            NaN           NaN                  N   \n",
      "freq                NaN            NaN           NaN            2975020   \n",
      "mean       1.362532e+00   3.847342e+00  1.497440e+00                NaN   \n",
      "min        0.000000e+00   0.000000e+00  1.000000e+00                NaN   \n",
      "25%        1.000000e+00   1.060000e+00  1.000000e+00                NaN   \n",
      "50%        1.000000e+00   1.800000e+00  1.000000e+00                NaN   \n",
      "75%        1.000000e+00   3.330000e+00  1.000000e+00                NaN   \n",
      "max        9.000000e+00   2.589281e+05  9.900000e+01                NaN   \n",
      "std        8.961200e-01   2.495838e+02  6.474767e+00                NaN   \n",
      "\n",
      "        PULocationID  DOLocationID  payment_type   fare_amount         extra  \\\n",
      "count   3.066766e+06  3.066766e+06  3.066766e+06  3.066766e+06  3.066766e+06   \n",
      "unique           NaN           NaN           NaN           NaN           NaN   \n",
      "top              NaN           NaN           NaN           NaN           NaN   \n",
      "freq             NaN           NaN           NaN           NaN           NaN   \n",
      "mean    1.663980e+02  1.643926e+02  1.194483e+00  1.836707e+01  1.537842e+00   \n",
      "min     1.000000e+00  1.000000e+00  0.000000e+00 -9.000000e+02 -7.500000e+00   \n",
      "25%     1.320000e+02  1.140000e+02  1.000000e+00  8.600000e+00  0.000000e+00   \n",
      "50%     1.620000e+02  1.620000e+02  1.000000e+00  1.280000e+01  1.000000e+00   \n",
      "75%     2.340000e+02  2.340000e+02  1.000000e+00  2.050000e+01  2.500000e+00   \n",
      "max     2.650000e+02  2.650000e+02  4.000000e+00  1.160100e+03  1.250000e+01   \n",
      "std     6.424413e+01  6.994368e+01  5.294762e-01  1.780782e+01  1.789592e+00   \n",
      "\n",
      "             mta_tax    tip_amount  tolls_amount  improvement_surcharge  \\\n",
      "count   3.066766e+06  3.066766e+06  3.066766e+06           3.066766e+06   \n",
      "unique           NaN           NaN           NaN                    NaN   \n",
      "top              NaN           NaN           NaN                    NaN   \n",
      "freq             NaN           NaN           NaN                    NaN   \n",
      "mean    4.882900e-01  3.367941e+00  5.184907e-01           9.820847e-01   \n",
      "min    -5.000000e-01 -9.622000e+01 -6.500000e+01          -1.000000e+00   \n",
      "25%     5.000000e-01  1.000000e+00  0.000000e+00           1.000000e+00   \n",
      "50%     5.000000e-01  2.720000e+00  0.000000e+00           1.000000e+00   \n",
      "75%     5.000000e-01  4.200000e+00  0.000000e+00           1.000000e+00   \n",
      "max     5.316000e+01  3.808000e+02  1.969900e+02           1.000000e+00   \n",
      "std     1.034641e-01  3.826759e+00  2.017579e+00           1.833529e-01   \n",
      "\n",
      "        total_amount  congestion_surcharge   airport_fee  \n",
      "count   3.066766e+06          2.995023e+06  2.995023e+06  \n",
      "unique           NaN                   NaN           NaN  \n",
      "top              NaN                   NaN           NaN  \n",
      "freq             NaN                   NaN           NaN  \n",
      "mean    2.702038e+01          2.274231e+00  1.074086e-01  \n",
      "min    -7.510000e+02         -2.500000e+00 -1.250000e+00  \n",
      "25%     1.540000e+01          2.500000e+00  0.000000e+00  \n",
      "50%     2.016000e+01          2.500000e+00  0.000000e+00  \n",
      "75%     2.870000e+01          2.500000e+00  0.000000e+00  \n",
      "max     1.169400e+03          2.500000e+00  1.250000e+00  \n",
      "std     2.216359e+01          7.718454e-01  3.556511e-01  \n"
     ]
    }
   ],
   "source": [
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "import os \n",
    "\n",
    "url = 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet'\n",
    "\n",
    "sample = pd.read_parquet(url, engine='pyarrow').head(5_000_000)\n",
    "\n",
    "print(sample.dtypes)\n",
    "print(sample.describe(include='all'))\n",
    "\n",
    "os.makedirs('./data', exist_ok=True)\n",
    "\n",
    "table = pa.Table.from_pandas(sample)\n",
    "pa.parquet.write_table(table, './data/yellow_tripdata_2023-01.snappy.parquet', compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/10 11:08:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Taxi\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "df_spark = spark.read.parquet('data/yellow_tripdata_2023-01.snappy.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de count() SEM cache: 1.29 segundos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de caching (incluindo 1º count()): 4.88 segundos\n",
      "Tempo de count() COM cache: 0.18 segundos\n",
      "\n",
      "Tempo de len() no Pandas: 0.0001 segundos\n",
      "\n",
      "Resumo:\n",
      "Spark (sem cache): 1.29 segundos\n",
      "Spark (com cache): 0.18 segundos\n",
      "Pandas: 0.0001 segundos\n",
      "Contagem Spark: 3066766\n",
      "Contagem Pandas: 500\n",
      "\n",
      "Status do cache (Spark):\n",
      "Disk Memory Deserialized 1x Replicated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/10 11:08:39 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_spark_count = time.time()\n",
    "spark_count = df_spark.count()\n",
    "spark_count_time = time.time() - start_spark_count\n",
    "print(f\"Tempo de count() SEM cache: {spark_count_time:.2f} segundos\")\n",
    "\n",
    "start_cache = time.time()\n",
    "df_spark.cache()  \n",
    "cached_count = df_spark.count() \n",
    "cache_time = time.time() - start_cache\n",
    "print(f\"Tempo de caching (incluindo 1º count()): {cache_time:.2f} segundos\")\n",
    "\n",
    "start_cached_count = time.time()\n",
    "spark_count_cached = df_spark.count()\n",
    "cached_count_time = time.time() - start_cached_count\n",
    "print(f\"Tempo de count() COM cache: {cached_count_time:.2f} segundos\")\n",
    "\n",
    "sample = pd.read_parquet('data/yellow_tripdata_2023-01.snappy.parquet').head(500)\n",
    "\n",
    "start_pandas = time.time()\n",
    "pandas_count = len(sample)\n",
    "pandas_time = time.time() - start_pandas\n",
    "print(f\"\\nTempo de len() no Pandas: {pandas_time:.4f} segundos\")\n",
    "\n",
    "print(\"\\nResumo:\")\n",
    "print(f\"Spark (sem cache): {spark_count_time:.2f} segundos\")\n",
    "print(f\"Spark (com cache): {cached_count_time:.2f} segundos\")\n",
    "print(f\"Pandas: {pandas_time:.4f} segundos\")\n",
    "print(f\"Contagem Spark: {spark_count}\")\n",
    "print(f\"Contagem Pandas: {pandas_count}\")\n",
    "\n",
    "print(\"\\nStatus do cache (Spark):\")\n",
    "print(df_spark.storageLevel) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2 — Pandas Profiling vs. Spark SQL Analysis\n",
    "- Generate a Pandas Profiling (ydata‑profiling) report on the 500 k taxi sample.\n",
    "- Recreate three key insights in Spark SQL (e.g., mean trip distance, 95‑th percentile fare).\n",
    "- Compare runtimes and memory; justify Pandas vs. Spark choices for typical HCIE workloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
