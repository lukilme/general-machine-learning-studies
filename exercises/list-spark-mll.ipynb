{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 — CSV ↔ Parquet Conversion & Basic EDA\n",
    "- Dataset: NYC Yellow Taxi 2023‑01 CSV  https://d37ci6vzurychx.cloudfront.net/tripdata/yellow_tripdata_2023-01.csv.gz\n",
    "- Load first 500 000 rows with Pandas for schema inspection and summary statistics\n",
    "(describe()).\n",
    "- Write the full file to Parquet with Snappy compression via Pandas (PyArrow backend).\n",
    "- Start a local Spark session, read the Parquet back, cache it, and verify matching row\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VendorID                          int64\n",
      "tpep_pickup_datetime     datetime64[us]\n",
      "tpep_dropoff_datetime    datetime64[us]\n",
      "passenger_count                 float64\n",
      "trip_distance                   float64\n",
      "RatecodeID                      float64\n",
      "store_and_fwd_flag               object\n",
      "PULocationID                      int64\n",
      "DOLocationID                      int64\n",
      "payment_type                      int64\n",
      "fare_amount                     float64\n",
      "extra                           float64\n",
      "mta_tax                         float64\n",
      "tip_amount                      float64\n",
      "tolls_amount                    float64\n",
      "improvement_surcharge           float64\n",
      "total_amount                    float64\n",
      "congestion_surcharge            float64\n",
      "airport_fee                     float64\n",
      "dtype: object\n",
      "            VendorID        tpep_pickup_datetime       tpep_dropoff_datetime  \\\n",
      "count   3.066766e+06                     3066766                     3066766   \n",
      "unique           NaN                         NaN                         NaN   \n",
      "top              NaN                         NaN                         NaN   \n",
      "freq             NaN                         NaN                         NaN   \n",
      "mean    1.730215e+00  2023-01-17 00:22:26.288164  2023-01-17 00:38:06.427874   \n",
      "min     1.000000e+00         2008-12-31 23:01:42         2009-01-01 14:29:11   \n",
      "25%     1.000000e+00  2023-01-09 16:21:57.250000         2023-01-09 16:37:06   \n",
      "50%     2.000000e+00  2023-01-17 08:42:29.500000  2023-01-17 08:58:30.500000   \n",
      "75%     2.000000e+00         2023-01-24 16:26:27         2023-01-24 16:42:49   \n",
      "max     2.000000e+00         2023-02-01 00:56:53         2023-02-02 09:28:47   \n",
      "std     4.438480e-01                         NaN                         NaN   \n",
      "\n",
      "        passenger_count  trip_distance    RatecodeID store_and_fwd_flag  \\\n",
      "count      2.995023e+06   3.066766e+06  2.995023e+06            2995023   \n",
      "unique              NaN            NaN           NaN                  2   \n",
      "top                 NaN            NaN           NaN                  N   \n",
      "freq                NaN            NaN           NaN            2975020   \n",
      "mean       1.362532e+00   3.847342e+00  1.497440e+00                NaN   \n",
      "min        0.000000e+00   0.000000e+00  1.000000e+00                NaN   \n",
      "25%        1.000000e+00   1.060000e+00  1.000000e+00                NaN   \n",
      "50%        1.000000e+00   1.800000e+00  1.000000e+00                NaN   \n",
      "75%        1.000000e+00   3.330000e+00  1.000000e+00                NaN   \n",
      "max        9.000000e+00   2.589281e+05  9.900000e+01                NaN   \n",
      "std        8.961200e-01   2.495838e+02  6.474767e+00                NaN   \n",
      "\n",
      "        PULocationID  DOLocationID  payment_type   fare_amount         extra  \\\n",
      "count   3.066766e+06  3.066766e+06  3.066766e+06  3.066766e+06  3.066766e+06   \n",
      "unique           NaN           NaN           NaN           NaN           NaN   \n",
      "top              NaN           NaN           NaN           NaN           NaN   \n",
      "freq             NaN           NaN           NaN           NaN           NaN   \n",
      "mean    1.663980e+02  1.643926e+02  1.194483e+00  1.836707e+01  1.537842e+00   \n",
      "min     1.000000e+00  1.000000e+00  0.000000e+00 -9.000000e+02 -7.500000e+00   \n",
      "25%     1.320000e+02  1.140000e+02  1.000000e+00  8.600000e+00  0.000000e+00   \n",
      "50%     1.620000e+02  1.620000e+02  1.000000e+00  1.280000e+01  1.000000e+00   \n",
      "75%     2.340000e+02  2.340000e+02  1.000000e+00  2.050000e+01  2.500000e+00   \n",
      "max     2.650000e+02  2.650000e+02  4.000000e+00  1.160100e+03  1.250000e+01   \n",
      "std     6.424413e+01  6.994368e+01  5.294762e-01  1.780782e+01  1.789592e+00   \n",
      "\n",
      "             mta_tax    tip_amount  tolls_amount  improvement_surcharge  \\\n",
      "count   3.066766e+06  3.066766e+06  3.066766e+06           3.066766e+06   \n",
      "unique           NaN           NaN           NaN                    NaN   \n",
      "top              NaN           NaN           NaN                    NaN   \n",
      "freq             NaN           NaN           NaN                    NaN   \n",
      "mean    4.882900e-01  3.367941e+00  5.184907e-01           9.820847e-01   \n",
      "min    -5.000000e-01 -9.622000e+01 -6.500000e+01          -1.000000e+00   \n",
      "25%     5.000000e-01  1.000000e+00  0.000000e+00           1.000000e+00   \n",
      "50%     5.000000e-01  2.720000e+00  0.000000e+00           1.000000e+00   \n",
      "75%     5.000000e-01  4.200000e+00  0.000000e+00           1.000000e+00   \n",
      "max     5.316000e+01  3.808000e+02  1.969900e+02           1.000000e+00   \n",
      "std     1.034641e-01  3.826759e+00  2.017579e+00           1.833529e-01   \n",
      "\n",
      "        total_amount  congestion_surcharge   airport_fee  \n",
      "count   3.066766e+06          2.995023e+06  2.995023e+06  \n",
      "unique           NaN                   NaN           NaN  \n",
      "top              NaN                   NaN           NaN  \n",
      "freq             NaN                   NaN           NaN  \n",
      "mean    2.702038e+01          2.274231e+00  1.074086e-01  \n",
      "min    -7.510000e+02         -2.500000e+00 -1.250000e+00  \n",
      "25%     1.540000e+01          2.500000e+00  0.000000e+00  \n",
      "50%     2.016000e+01          2.500000e+00  0.000000e+00  \n",
      "75%     2.870000e+01          2.500000e+00  0.000000e+00  \n",
      "max     1.169400e+03          2.500000e+00  1.250000e+00  \n",
      "std     2.216359e+01          7.718454e-01  3.556511e-01  \n"
     ]
    }
   ],
   "source": [
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "import os \n",
    "\n",
    "url = 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet'\n",
    "\n",
    "sample = pd.read_parquet(url, engine='pyarrow').head(5_000_000)\n",
    "\n",
    "print(sample.dtypes)\n",
    "print(sample.describe(include='all'))\n",
    "\n",
    "os.makedirs('./data', exist_ok=True)\n",
    "\n",
    "table = pa.Table.from_pandas(sample)\n",
    "pa.parquet.write_table(table, './data/yellow_tripdata_2023-01.snappy.parquet', compression='snappy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/10 12:01:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Taxi\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "df_spark = spark.read.parquet('data/yellow_tripdata_2023-01.snappy.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de count() SEM cache: 1.57 segundos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de caching (incluindo 1º count()): 5.55 segundos\n",
      "Tempo de count() COM cache: 0.19 segundos\n",
      "\n",
      "Tempo de len() no Pandas: 0.0001 segundos\n",
      "\n",
      "Resumo:\n",
      "Spark (sem cache): 1.57 segundos\n",
      "Spark (com cache): 0.19 segundos\n",
      "Pandas: 0.0001 segundos\n",
      "Contagem Spark: 3066766\n",
      "Contagem Pandas: 500\n",
      "\n",
      "Status do cache (Spark):\n",
      "Disk Memory Deserialized 1x Replicated\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_spark_count = time.time()\n",
    "spark_count = df_spark.count()\n",
    "spark_count_time = time.time() - start_spark_count\n",
    "print(f\"Tempo de count() SEM cache: {spark_count_time:.2f} segundos\")\n",
    "\n",
    "start_cache = time.time()\n",
    "df_spark.cache()  \n",
    "cached_count = df_spark.count() \n",
    "cache_time = time.time() - start_cache\n",
    "print(f\"Tempo de caching (incluindo 1º count()): {cache_time:.2f} segundos\")\n",
    "\n",
    "start_cached_count = time.time()\n",
    "spark_count_cached = df_spark.count()\n",
    "cached_count_time = time.time() - start_cached_count\n",
    "print(f\"Tempo de count() COM cache: {cached_count_time:.2f} segundos\")\n",
    "\n",
    "sample = pd.read_parquet('data/yellow_tripdata_2023-01.snappy.parquet').head(500)\n",
    "\n",
    "start_pandas = time.time()\n",
    "pandas_count = len(sample)\n",
    "pandas_time = time.time() - start_pandas\n",
    "print(f\"\\nTempo de len() no Pandas: {pandas_time:.4f} segundos\")\n",
    "\n",
    "print(\"\\nResumo:\")\n",
    "print(f\"Spark (sem cache): {spark_count_time:.2f} segundos\")\n",
    "print(f\"Spark (com cache): {cached_count_time:.2f} segundos\")\n",
    "print(f\"Pandas: {pandas_time:.4f} segundos\")\n",
    "print(f\"Contagem Spark: {spark_count}\")\n",
    "print(f\"Contagem Pandas: {pandas_count}\")\n",
    "\n",
    "print(\"\\nStatus do cache (Spark):\")\n",
    "print(df_spark.storageLevel) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2 — Pandas Profiling vs. Spark SQL Analysis\n",
    "- Generate a Pandas Profiling (ydata‑profiling) report on the 500 k taxi sample.\n",
    "- Recreate three key insights in Spark SQL (e.g., mean trip distance, 95‑th percentile fare).\n",
    "- Compare runtimes and memory; justify Pandas vs. Spark choices for typical HCIE workloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <ins><a href=\"https://ydata.ai/register\">Upgrade to ydata-sdk</a></ins>\n",
       "                <p>\n",
       "                    Improve your data and profiling with ydata-sdk, featuring data quality scoring, redundancy detection, outlier identification, text validation, and synthetic data generation.\n",
       "                </p>\n",
       "            </div>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0faa534441834c8c9d52b9d45a196133",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 166.00it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f2a71d6bd9444a88bc5b8170685eadd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27c7dc5a69ac4726bb61769307c5ad88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a751ad1f90f1490990424abc9176a9ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "profile = ProfileReport(sample, title=\"NYC Taxi Sample Profiling\", explorative=True)\n",
    "profile.to_file(\"./data/nyc_taxi_sample_report.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|mean_trip_distance|\n",
      "+------------------+\n",
      "| 3.847342030660236|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "view_spark = df_spark.createOrReplaceTempView(\"taxi_data\")\n",
    "\n",
    "spark.sql(\"SELECT AVG(trip_distance) AS mean_trip_distance \\\n",
    "    FROM taxi_data\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|fare_95th_percentile|\n",
      "+--------------------+\n",
      "|                65.3|\n",
      "+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT percentile_approx(fare_amount, 0.95) AS fare_95th_percentile \\\n",
    "    FROM taxi_data\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+\n",
      "|PULocationID|trip_count|\n",
      "+------------+----------+\n",
      "|         132|    160030|\n",
      "|         237|    148074|\n",
      "|         236|    138391|\n",
      "|         161|    135417|\n",
      "|         186|    109227|\n",
      "+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT PULocationID, COUNT(*) AS trip_count \\\n",
    "            FROM taxi_data \\\n",
    "            GROUP BY PULocationID \\\n",
    "            ORDER BY trip_count DESC \\\n",
    "            LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 — Data Cleaning Pipeline with Pandas UDFs\n",
    "- Create a Spark DataFrame of the full taxi dataset.\n",
    "- Implement a scalar Pandas UDF that buckets trip_distance into quartiles(Q1–Q4).\n",
    "- Assemble an ML Pipeline: Bucketizer (UDF output), StringIndexer(payment_type), VectorAssembler.\n",
    "- Save the clean DataFrame as an HCFS table for future exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 42:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+------+\n",
      "|trip_distance_quartile| count|\n",
      "+----------------------+------+\n",
      "|                    Q1|777435|\n",
      "|                    Q2|762277|\n",
      "|                    Q3|763202|\n",
      "|                    Q4|763852|\n",
      "+----------------------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "@pandas_udf(StringType())\n",
    "def bucket_trip_distance(trip_distances: pd.Series) -> pd.Series:\n",
    "    q1 = trip_distances.quantile(0.25)\n",
    "    q2 = trip_distances.quantile(0.50)\n",
    "    q3 = trip_distances.quantile(0.75)\n",
    "    return pd.cut(\n",
    "        trip_distances,\n",
    "        bins=[-float(\"inf\"), q1, q2, q3, float(\"inf\")],\n",
    "        labels = [\"Q1\", \"Q2\", \"Q3\", \"Q4\"]\n",
    "    )\n",
    "\n",
    "df = df_spark.withColumn(\"trip_distance_quartile\", bucket_trip_distance(df_spark[\"trip_distance\"]))\n",
    "\n",
    "df.groupBy(\"trip_distance_quartile\").count().orderBy(\"trip_distance_quartile\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "\n",
    "payment_indexer = StringIndexer(\n",
    "    inputCol=\"payment_type\",\n",
    "    outputCol=\"payment_type_index\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"trip_distance_quartile\", \"payment_type_index\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[payment_indexer, assembler])\n",
    "\n",
    "model = pipeline.fit(df)\n",
    "df_transformed = model.transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df.write.mode(\"overwrite\").saveAsTable(\"cleaned_taxi_data\")\n",
    "except Exception as e:\n",
    "    print(f\"Table save failed, falling back to directory save: {e}\")\n",
    "    df.write.mode(\"overwrite\").parquet(\"output/cleaned_taxi_data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4 — Basic Regression with MLlib’s LinearRegression\n",
    "- Predict total_amount using engineered features.\n",
    "- Train/test split 80/20; evaluate RMSE.\n",
    "- Collect 1 % of predictions to Pandas, plot residuals with Matplotlib, and detect heteroscedasticity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "root\n",
      " |-- VendorID: long (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- passenger_count: double (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: double (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- airport_fee: double (nullable = true)\n",
      " |-- trip_distance_quartile: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(type(df))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/10 12:23:20 WARN Instrumentation: [9194deaf] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/05/10 12:24:15 WARN MemoryStore: Not enough space to cache rdd_13_0 in memory! (computed 118.2 MiB so far)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 21.598327934751072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19935/2951972457.py:36: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "feature_cols = [\n",
    "    'trip_distance',\n",
    "    'passenger_count',\n",
    "    'payment_type',\n",
    "    'PULocationID',\n",
    "    'DOLocationID',\n",
    "    'RatecodeID'\n",
    "]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features',handleInvalid=\"skip\")\n",
    "df_prepared = assembler.transform(df)\n",
    "train_data, test_data = df_prepared.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "lr = LinearRegression(featuresCol='features', labelCol='total_amount')\n",
    "lr_model = lr.fit(train_data)\n",
    "predictions = lr_model.transform(test_data)\n",
    "evaluator = RegressionEvaluator(labelCol='total_amount', predictionCol='prediction', metricName='rmse')\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE) on test data = {rmse}\")\n",
    "\n",
    "sample_predictions = predictions.sample(fraction=0.01, seed=42)\n",
    "\n",
    "sample_pd = sample_predictions.select('prediction', 'total_amount').toPandas()\n",
    "sample_pd['residuals'] = sample_pd['total_amount'] - sample_pd['prediction']\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(sample_pd['prediction'], sample_pd['residuals'], alpha=0.5)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted Total Amount')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals vs Predicted Total Amount')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303.5\n",
      "-74.0\n",
      "Intercepto: 44.24210836689825\n",
      "Coeficientes: [0.03992139905793312,0.7897287066271028,-6.96582345641627,-0.03575762205170507,-0.02600751345190344,0.17585689457135667]\n",
      "trip_distance: 0.03992139905793312\n",
      "passenger_count: 0.7897287066271028\n",
      "payment_type: -6.96582345641627\n",
      "PULocationID: -0.03575762205170507\n",
      "DOLocationID: -0.02600751345190344\n",
      "RatecodeID: 0.17585689457135667\n"
     ]
    }
   ],
   "source": [
    "print(sample_pd['total_amount'].max())\n",
    "print(sample_pd['total_amount'].min())\n",
    "coefficients = lr_model.coefficients\n",
    "intercept = lr_model.intercept\n",
    "\n",
    "print(f\"Intercepto: {intercept}\")\n",
    "print(f\"Coeficientes: {coefficients}\")\n",
    "\n",
    "for feature, coef in zip(feature_cols, coefficients):\n",
    "    print(f\"{feature}: {coef}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A equação de regressão pode ser expressa como:\n",
    "\n",
    "$$\n",
    "\\text{total\\_amount} = \\beta_0 + \\beta_1 \\times \\text{trip\\_distance} + \\beta_2 \\times \\text{fare\\_amount} + \\cdots + \\epsilon\n",
    "$$\n",
    "\n",
    "Onde:\n",
    "- $\\beta_0$ é o intercepto\n",
    "- $\\beta_1$, $\\beta_2$ são os coeficientes\n",
    "- $\\epsilon$ representa o erro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5 — Hyper‑Parameter Tuning with CrossValidator\n",
    "- Grid‑search regParam ∈ {0.01, 0.1, 0.5} and elasticNetParam ∈ {0, 0.5, 1}.\n",
    "- Use 3‑fold CV and RegressionEvaluator (metric = r2).\n",
    "- Export cvModel.avgMetrics to Pandas and build a Seaborn barplot of R² vs. hyper‑parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/10 12:43:27 WARN MemoryStore: Not enough space to cache rdd_265_1 in memory! (computed 38.1 MiB so far)\n",
      "25/05/10 12:43:27 WARN BlockManager: Persisting block rdd_265_1 to disk instead.\n",
      "25/05/10 12:43:27 WARN MemoryStore: Not enough space to cache rdd_265_3 in memory! (computed 73.8 MiB so far)\n",
      "25/05/10 12:43:27 WARN BlockManager: Persisting block rdd_265_3 to disk instead.\n",
      "25/05/10 12:43:27 WARN MemoryStore: Not enough space to cache rdd_265_0 in memory! (computed 73.9 MiB so far)\n",
      "25/05/10 12:43:27 WARN BlockManager: Persisting block rdd_265_0 to disk instead.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "lr = LinearRegression(featuresCol='features', labelCol='total_amount')\n",
    "\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol='total_amount',\n",
    "    predictionCol='prediction',\n",
    "    metricName='r2'\n",
    ")\n",
    "\n",
    "# paramGrid = ParamGridBuilder() \\\n",
    "#     .addGrid(lr.regParam, [0.01, 0.1, 0.5]) \\\n",
    "#     .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "#     .build()\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.5]) \\\n",
    "    .addGrid(lr.elasticNetParam, [1.0]) \\\n",
    "    .build()\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "\n",
    "cv = CrossValidator(estimator=lr,\n",
    "                    estimatorParamMaps=paramGrid,\n",
    "                    evaluator=evaluator,\n",
    "                    numFolds=3)\n",
    "\n",
    "cvModel = cv.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhor regParam: 0.5\n",
      "Melhor elasticNetParam: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19935/1704575547.py:28: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "avg_metrics = cvModel.avgMetrics\n",
    "best_model = cvModel.bestModel\n",
    "\n",
    "best_reg_param = best_model._java_obj.getRegParam()\n",
    "best_elastic_net_param = best_model._java_obj.getElasticNetParam()\n",
    "\n",
    "print(f\"Melhor regParam: {best_reg_param}\")\n",
    "print(f\"Melhor elasticNetParam: {best_elastic_net_param}\")\n",
    "\n",
    "metrics_data = []\n",
    "for params, r2 in zip(paramGrid, avg_metrics):\n",
    "    reg = params[lr.regParam]\n",
    "    elastic = params[lr.elasticNetParam]\n",
    "    metrics_data.append((reg, elastic, r2))\n",
    "\n",
    "df_metrics = pd.DataFrame(metrics_data, columns=['regParam', 'elasticNetParam', 'r2'])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=df_metrics, x='regParam', y='r2', hue='elasticNetParam')\n",
    "plt.title('R² por Combinação de Hiperparâmetros')\n",
    "plt.xlabel('regParam')\n",
    "plt.ylabel('R²')\n",
    "plt.legend(title='elasticNetParam')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6 — Classification with RandomForestClassifier\n",
    "- Define binary label: high‑tip vs. low‑tip (tip ≥ 15 % of fare).\n",
    "- Train RandomForestClassifier with 100 trees; retrieve feature importances.\n",
    "- Collect importances to Pandas and visualise top‑10 predictors; discuss relevance to feature pruning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"high_tip\",\n",
    "    when(col(\"tip_amount\") / col(\"fare_amount\") >= 0.15, 1).otherwise(0)\n",
    ")\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"payment_type\", outputCol=\"payment_type_indexed\")\n",
    "\n",
    "feature_cols = ['trip_distance', 'passenger_count', 'fare_amount', 'payment_type_indexed']\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "rf = RandomForestClassifier(labelCol=\"high_tip\", featuresCol=\"features\", numTrees=100, seed=42)\n",
    "\n",
    "pipeline = Pipeline(stages=[indexer, assembler, rf])\n",
    "\n",
    "model = pipeline.fit(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
